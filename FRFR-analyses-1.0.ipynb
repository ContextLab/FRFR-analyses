{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature-rich free recall analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from __future__ import division\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db_url = \"sqlite:///participants.db\"\n",
    "table_name = 'turkdemo'\n",
    "data_column_name = 'datastring'\n",
    "\n",
    "# boilerplace sqlalchemy setup\n",
    "engine = create_engine(db_url)\n",
    "metadata = MetaData()\n",
    "metadata.bind = engine\n",
    "table = Table(table_name, metadata, autoload=True)\n",
    "\n",
    "# make a query and loop through\n",
    "s = table.select()\n",
    "rows = s.execute()\n",
    "\n",
    "data = []\n",
    "for row in rows:\n",
    "    data.append(row[data_column_name])\n",
    "    \n",
    "# Now we have all participant datastrings in a list.\n",
    "# Let's make it a bit easier to work with:\n",
    "\n",
    "# parse each participant's datastring as json object\n",
    "# and take the 'data' sub-object\n",
    "data = [json.loads(part)['data'] for part in data if part is not None]\n",
    "\n",
    "# insert uniqueid field into trialdata in case it wasn't added\n",
    "# in experiment:\n",
    "for part in data:\n",
    "    for record in part:\n",
    "#         print(record)\n",
    "        if type(record['trialdata']) is list:\n",
    "\n",
    "            record['trialdata'] = {record['trialdata'][0]:record['trialdata'][1]}\n",
    "        record['trialdata']['uniqueid'] = record['uniqueid']\n",
    "        \n",
    "# flatten nested list so we just have a list of the trialdata recorded\n",
    "# each time psiturk.recordTrialData(trialdata) was called.\n",
    "def isNotNumber(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return True\n",
    "\n",
    "data = [record['trialdata'] for part in data for record in part]\n",
    "\n",
    "# filter out fields that we dont want using isNotNumber function\n",
    "filtered_data = [{k:v for (k,v) in part.items() if isNotNumber(k)} for part in data]\n",
    "    \n",
    "# Put all subjects' trial data into a dataframe object from the\n",
    "# 'pandas' python library: one option among many for analysis\n",
    "data_frame = pd.DataFrame(filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a column to keep track of experiment version number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db_url = \"sqlite:///participants.db\"\n",
    "table_name = 'turkdemo'\n",
    "data_column_name = 'codeversion'\n",
    "\n",
    "# boilerplace sqlalchemy setup\n",
    "engine = create_engine(db_url)\n",
    "metadata = MetaData()\n",
    "metadata.bind = engine\n",
    "table = Table(table_name, metadata, autoload=True)\n",
    "\n",
    "# make a query and loop through\n",
    "s = table.select()\n",
    "rows = s.execute()\n",
    "\n",
    "versions = []\n",
    "for row in rows:\n",
    "    versions.append(row[data_column_name])\n",
    "    \n",
    "version_col = []\n",
    "for idx,sub in enumerate(data_frame['uniqueid'].unique()):\n",
    "    for i in range(sum(data_frame['uniqueid']==sub)):\n",
    "        version_col.append(versions[idx])\n",
    "data_frame['exp_version']=version_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of subjects in each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subids = list(data_frame[data_frame['listNumber']==15]['uniqueid'].unique())\n",
    "\n",
    "d = dict()\n",
    "for sub in subids:\n",
    "    key = data_frame[data_frame['uniqueid']==sub]['exp_version'].values[0]\n",
    "    if key in d:\n",
    "        d[key] += 1\n",
    "    else:\n",
    "        d[key] = 1\n",
    "print('Here is a count of how many subjects we have in each experiment: ',d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in word pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in stimulus library\n",
    "wordpool = pd.read_csv('stimuli/cut_wordpool.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function takes the data frame and returns subject specific data based on the subid variable\n",
    "def filterData(data_frame,subid):\n",
    "    filtered_stim_data = data_frame[data_frame['stimulus'].notnull() & data_frame['listNumber'].notnull()]\n",
    "    filtered_stim_data = filtered_stim_data[filtered_stim_data['trial_type']=='single-stim']\n",
    "    filtered_stim_data =  filtered_stim_data[filtered_stim_data['uniqueid']==subid]\n",
    "    return filtered_stim_data\n",
    "\n",
    "# this function parses the data creating an array of dictionaries, where each dictionary represents a trial (word presented) along with the stimulus attributes\n",
    "def createStimDict(data):\n",
    "    stimDict = []\n",
    "    for index, row in data.iterrows():\n",
    "        stimDict.append({\n",
    "                'text': str(re.findall('>(.+)<',row['stimulus'])[0]),\n",
    "                'color' : { 'r' : int(re.findall('rgb\\((.+)\\)',row['stimulus'])[0].split(',')[0]),\n",
    "                           'g' : int(re.findall('rgb\\((.+)\\)',row['stimulus'])[0].split(',')[1]),\n",
    "                           'b' : int(re.findall('rgb\\((.+)\\)',row['stimulus'])[0].split(',')[2])\n",
    "                           },\n",
    "                'location' : {\n",
    "                    'top': float(re.findall('top:(.+)\\%;', row['stimulus'])[0]),\n",
    "                    'left' : float(re.findall('left:(.+)\\%', row['stimulus'])[0])\n",
    "                    },\n",
    "                'category' : wordpool['CATEGORY'].iloc[list(wordpool['WORD'].values).index(str(re.findall('>(.+)<',row['stimulus'])[0]))],\n",
    "                'size' : wordpool['SIZE'].iloc[list(wordpool['WORD'].values).index(str(re.findall('>(.+)<',row['stimulus'])[0]))],\n",
    "                'wordLength' : len(str(re.findall('>(.+)<',row['stimulus'])[0])),\n",
    "                'firstLetter' : str(re.findall('>(.+)<',row['stimulus'])[0])[0],\n",
    "                'listnum' : row['listNumber']\n",
    "            })\n",
    "    return stimDict\n",
    "\n",
    "# this function loads in the recall data into an array of arrays, where each array represents a list of words\n",
    "def loadRecallData(subid):\n",
    "    recalledWords = []\n",
    "    for i in range(0,16):\n",
    "        try:\n",
    "            f = open('recall_data/' + subid + '-' + str(i) + '.wav.txt', 'rb')\n",
    "            try:\n",
    "                spamreader = csv.reader(f, delimiter=' ', quotechar='|')\n",
    "            except:\n",
    "                f = open('recall_data/' + subid + '/' + subid + '-' + str(i) + '.wav.txt', 'rb')\n",
    "                spamreader = csv.reader(f, delimiter=' ', quotechar='|')\n",
    "        except (IOError, OSError) as e:\n",
    "            print(e)\n",
    "        for row in spamreader:\n",
    "            recalledWords.append(row[0].split(','))\n",
    "    return recalledWords\n",
    "\n",
    "# this function computes accuracy for a series of lists\n",
    "def computeListAcc(stimDict,recalledWords):\n",
    "    accVec = []\n",
    "    for i in range(0,16):\n",
    "        stim = [stim['text'] for stim in stimDict if stim['listnum']==i]\n",
    "        recalled= recalledWords[i]\n",
    "        \n",
    "        acc = 0\n",
    "        tmpstim = stim[:]\n",
    "        for word in recalled:\n",
    "            if word in tmpstim:\n",
    "                tmpstim.remove(word)\n",
    "                acc+=1\n",
    "        accVec.append(acc/len(stim))\n",
    "    return accVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define fingerprint class (this will be moved to an importable module at some point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class that computes the fingerprint based on stimulus features and recall organization\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Pyfingerprint(object):\n",
    "    '''pyfingerprint module'''\n",
    "\n",
    "    def __init__(self, state=None, features=['category', 'size', 'firstLetter', 'wordLength', 'location', 'color', 'temporal'], weights=None, alpha=4, tau=1, sortby=None):\n",
    "        self.state = state\n",
    "        self.features = features\n",
    "        self.weights = weights\n",
    "        self.alpha = alpha\n",
    "        self.tau = tau\n",
    "        self.sortby = sortby\n",
    "\n",
    "    #### public functions ####\n",
    "\n",
    "    # given a stimulus list and recalled words, compute the weights\n",
    "    def computeWeights(self, currentList, recalledWords):\n",
    "        currentList = self._computeDistance(currentList)\n",
    "        return self._computeFeatureWeights(currentList, recalledWords, self.features)\n",
    "\n",
    "    def updateWeights(self,newWeights):\n",
    "        if self.weights is not None:\n",
    "            print('weights exist, updating..')\n",
    "            for feature in self.weights: \n",
    "                self.weights[feature].append(newWeights[feature]);\n",
    "        else:\n",
    "            print('new weights..')\n",
    "            self.weights = {};\n",
    "            for feature in newWeights:\n",
    "                self.weights[feature] = [];\n",
    "                self.weights[feature].append(newWeights[feature]);\n",
    "        print('weights: ', self.weights)\n",
    "\n",
    "    def getReorderedList(self,nextList):\n",
    "        print('Reordering list according to state: ' + str(self.state))\n",
    "        if self.state == 'feature-based':\n",
    "            return _featurizeList(nextList)\n",
    "        elif self.state == 'random':\n",
    "            return _randomizeList(nextList)\n",
    "        elif self.state == 'optimal':\n",
    "            return _optimizeList(nextList)\n",
    "        elif self.state == 'opposite':\n",
    "            return _oppositizeList(nextList)\n",
    "        elif self.state == 'strip-features':\n",
    "            return _stripFeatures(nextList)\n",
    "        else:\n",
    "            print('Warning: No fingerprint state assigned, returning same list..')\n",
    "            return nextList\n",
    "        \n",
    "    #### private functions ####\n",
    "    \n",
    "    def _computeDistance(self,stimArray):\n",
    "        \n",
    "        # initialize distance dictionary\n",
    "        for stimulus in stimArray:\n",
    "            stimulus['distances'] = {}\n",
    "            for feature in self.features:\n",
    "                stimulus['distances'][feature] = []\n",
    "                \n",
    "        # loop over the lists to create distance matrices\n",
    "        for i,stimulus1 in enumerate(stimArray):\n",
    "            for j,stimulus2 in enumerate(stimArray):\n",
    "                \n",
    "                # logic for temporal clustering\n",
    "                stimArray[i]['distances']['temporal'].append({\n",
    "                        'word' : stimArray[j]['text'],\n",
    "                        'dist' : abs(i - j)\n",
    "                    })\n",
    "                \n",
    "                # logic for category, need to add if statement if we are using category as a feature\n",
    "                stimArray[i]['distances']['category'].append({\n",
    "                        'word' : stimArray[j]['text'],\n",
    "                        'dist' : int(stimArray[i]['category'] != stimArray[j]['category'])\n",
    "                    })\n",
    "\n",
    "                # logic for size\n",
    "                stimArray[i]['distances']['size'].append({\n",
    "                    'word': stimArray[j]['text'],\n",
    "                    'dist': int(stimArray[i]['size'] != stimArray[j]['size'])\n",
    "                })\n",
    "\n",
    "                # logic for first letter\n",
    "                stimArray[i]['distances']['firstLetter'].append({\n",
    "                    'word': stimArray[j]['text'],\n",
    "                    'dist': int(stimArray[i]['firstLetter'] != stimArray[j]['firstLetter'])\n",
    "                })\n",
    "\n",
    "                # logic for word length\n",
    "                stimArray[i]['distances']['wordLength'].append({\n",
    "                    'word': stimArray[j]['text'],\n",
    "                    'dist': abs(stimArray[i]['wordLength'] - stimArray[j]['wordLength'])\n",
    "                });\n",
    "\n",
    "                # logic for color distance\n",
    "                stimArray[i]['distances']['color'].append({\n",
    "                    'word': stimArray[j]['text'],\n",
    "                    'dist': math.sqrt(math.pow(stimArray[i]['color']['r'] - stimArray[j]['color']['r'], 2) + math.pow(stimArray[i]['color']['g'] - stimArray[j]['color']['g'], 2) +\n",
    "                        math.pow(stimArray[i]['color']['b'] - stimArray[j]['color']['b'], 2))\n",
    "                });\n",
    "\n",
    "                # logic for spatial distance\n",
    "                stimArray[i]['distances']['location'].append({\n",
    "                    'word': stimArray[j]['text'],\n",
    "                    'dist': math.sqrt(pow(stimArray[i]['location']['top'] - stimArray[j]['location']['top'], 2) + pow(stimArray[i]['location']['left'] - stimArray[j]['location']['left'], 2))\n",
    "                })\n",
    "                \n",
    "        return stimArray\n",
    "    \n",
    "    def _computeFeatureWeights(self,currentList, recalledWords, features):\n",
    "\n",
    "        # initialize the weights object for just this list\n",
    "        listWeights = {}\n",
    "        for feature in self.features:\n",
    "            listWeights[feature] = []\n",
    "\n",
    "        # return default list if there is not enough data to compute the fingerprint\n",
    "        if len(recalledWords) <= 2:\n",
    "            print('Not enough recalls to compute fingerprint, returning default fingerprint.. (everything is .5)')\n",
    "            for feature in features:\n",
    "                listWeights[feature] = .5\n",
    "            return listWeights\n",
    "        \n",
    "        # initialize pastWords list\n",
    "        pastWords = []\n",
    "\n",
    "        # finger print analysis\n",
    "        for i in range(0,len(recalledWords)-1):\n",
    "\n",
    "            # grab current word\n",
    "            currentWord = recalledWords[i]\n",
    "\n",
    "            # grab the next word\n",
    "            nextWord = recalledWords[i + 1]\n",
    "            \n",
    "            # grab the words from the encoding list\n",
    "            encodingWords = [stimulus['text'] for stimulus in currentList]\n",
    "            \n",
    "            # append current word to past words log\n",
    "            # pastWords.append(currentWord)\n",
    "            \n",
    "            # if both recalled words are in the encoding list\n",
    "            if (currentWord in encodingWords and nextWord in encodingWords) and (currentWord not in pastWords and nextWord not in pastWords): \n",
    "                # print(currentWord,nextWord,encodingWords,pastWords)\n",
    "                \n",
    "\n",
    "                for feature in features:\n",
    "\n",
    "                    # get the distance vector for the current word\n",
    "                    distVec = currentList[encodingWords.index(currentWord)]['distances'][feature]\n",
    "\n",
    "                    # filter distVec removing the words that have already been analyzed from future calculations\n",
    "                    filteredDistVec = []\n",
    "                    for word in distVec:\n",
    "                        if word['word'] in pastWords:\n",
    "                            pass\n",
    "                        else:\n",
    "                            filteredDistVec.append(word)\n",
    "                            \n",
    "\n",
    "                    # sort distWords by distances\n",
    "                    filteredDistVec = sorted(filteredDistVec, key=lambda item:item['dist'])\n",
    "                    \n",
    "                    # compute the category listWeights\n",
    "                    nextWordIdx = [word['word'] for word in filteredDistVec].index(nextWord)\n",
    "\n",
    "                    # not sure about this part\n",
    "                    idxs = []\n",
    "                    for idx,word in enumerate(filteredDistVec):\n",
    "                        if filteredDistVec[nextWordIdx]['dist'] == word['dist']:\n",
    "                            idxs.append(idx)\n",
    "\n",
    "                    listWeights[feature].append(1 - (sum(idxs)/len(idxs) / len(filteredDistVec)))\n",
    "\n",
    "                pastWords.append(currentWord)\n",
    "\n",
    "        for feature in listWeights:\n",
    "            listWeights[feature] = np.mean(listWeights[feature])\n",
    "\n",
    "        return listWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subjects who have completed the exp\n",
    "subids = list(data_frame[data_frame['listNumber']==15]['uniqueid'].unique())\n",
    "\n",
    "# issue with this subject - need to look into it further\n",
    "subids.remove('debugGPNALW:debugXSJ1FD')\n",
    "subids.remove('debug4PXFJG:debug3V9BT9')\n",
    "subids.remove('debugAD2211:debugB3TKJQ')\n",
    "\n",
    "# for each subject that completed the experiment\n",
    "for idx,sub in enumerate(subids):\n",
    "    \n",
    "    print('Running analysis for subject: ', sub)    \n",
    "        \n",
    "    # get the subjects data\n",
    "    filteredStimData = filterData(data_frame,sub)\n",
    "    \n",
    "    # parse the subjects data\n",
    "    stimDict = createStimDict(filteredStimData)\n",
    "    \n",
    "    # load in the recall data\n",
    "    recalledWords = loadRecallData(sub)\n",
    "    \n",
    "    # initialize the fingerprint\n",
    "    pyfingerprint = Pyfingerprint()\n",
    "    fingerprints= []\n",
    "    \n",
    "    # compute a fingerprint for each list\n",
    "    for i in range(0,16):\n",
    "        fingerprints.append(pyfingerprint.computeWeights([stim for stim in stimDict if stim['listnum']==i],recalledWords[i]))\n",
    "        fingerprints[i]['listNum']=i\n",
    "    tmp = pd.DataFrame(fingerprints)\n",
    "    \n",
    "    # compute accuracy\n",
    "    accVec = computeListAcc(stimDict,recalledWords)\n",
    "    \n",
    "    # organize the data\n",
    "    tmp['accuracy']=accVec\n",
    "    tmp['subId']=idx\n",
    "    tmp['experiment']=filteredStimData['exp_version'].values[0]\n",
    "    cols = ['experiment','subId','listNum','category','color','firstLetter','location','size','wordLength','temporal','accuracy']\n",
    "    \n",
    "    if idx==0:\n",
    "        fingerprintsDF = tmp[cols]\n",
    "    else:\n",
    "        fingerprintsDF = fingerprintsDF.append(tmp[cols],ignore_index=True)\n",
    "\n",
    "fingerprintsDF['experiment'] = fingerprintsDF['experiment'].replace('0.0','1.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganize the data to play nice with seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newData = []\n",
    "for index, row in fingerprintsDF.iterrows():\n",
    "    for feature in ['category','color','firstLetter','location','size','wordLength','temporal']:\n",
    "        newData.append({\n",
    "                'experiment': row['experiment'],\n",
    "                'subId': row['subId'],\n",
    "                'listNum': row['listNum'],\n",
    "                'feature': feature,\n",
    "                'accuracy': row['accuracy'],\n",
    "                'value': row[feature]\n",
    "            })\n",
    "fingerprintsDF = pd.DataFrame(newData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries and config for seaborn and statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import ttest_ind as ttest\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=2, rc={\"lines.linewidth\": 2.5})\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avg accuracy for exp 1 and 2 together - all lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "exp1 = fingerprintsDF['experiment']=='1.1'\n",
    "exp2 = fingerprintsDF['experiment']=='2.1'\n",
    "data = fingerprintsDF[(exp1 | exp2)]\n",
    "data = data.groupby(['subId','experiment']).mean().reset_index(level=['experiment'])\n",
    "ax = sns.violinplot(y='accuracy',x='experiment', data=data)\n",
    "plt.ylabel('Proportion of words recalled')\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n",
    "\n",
    "exp1 = data[data['experiment']=='1.1']['accuracy']\n",
    "exp2 = data[data['experiment']=='2.1']['accuracy']\n",
    "ttest(exp1,exp2)\n",
    "\n",
    "# plt.savefig('avgAcc_exp1&2.pdf',format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avg accuracy for exp 1 and 2 together - first half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "exp1 = fingerprintsDF['experiment']=='1.1'\n",
    "exp2 = fingerprintsDF['experiment']=='2.1'\n",
    "firsthalf = fingerprintsDF['listNum']<8\n",
    "data = fingerprintsDF[(exp1 | exp2) & firsthalf].groupby(['subId','experiment']).mean().reset_index(level=['experiment'])\n",
    "ax = sns.violinplot(y='accuracy',x='experiment', data=data)\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel('Proportion of words recalled')\n",
    "plt.show()\n",
    "\n",
    "exp1 = data[data['experiment']=='1.1']['accuracy']\n",
    "exp2 = data[data['experiment']=='2.1']['accuracy']\n",
    "ttest(exp1,exp2)\n",
    "\n",
    "# plt.savefig('avgAcc_exp1&2_firstHalf.pdf',format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avg accuracy for exp 1 and 2 together - second half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "exp1 = fingerprintsDF['experiment']=='1.1'\n",
    "exp2 = fingerprintsDF['experiment']=='2.1'\n",
    "secondhalf = fingerprintsDF['listNum']>7\n",
    "data = fingerprintsDF[(exp1 | exp2) & secondhalf].groupby(['subId','experiment']).mean().reset_index(level=['experiment'])\n",
    "ax = sns.violinplot(y='accuracy',x='experiment', data=data)\n",
    "plt.ylabel('Proportion of words recalled')\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n",
    "\n",
    "exp1 = data[data['experiment']=='1.1']['accuracy']\n",
    "exp2 = data[data['experiment']=='2.1']['accuracy']\n",
    "ttest(exp1,exp2)\n",
    "\n",
    "# plt.savefig('avgAcc_exp1&2_secondHalf.pdf',format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avg fingerprint for exp 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "exp1 = fingerprintsDF['experiment']=='1.1'\n",
    "data = fingerprintsDF[exp1].groupby(['subId','experiment','feature']).mean().reset_index(level=['experiment','feature'])\n",
    "ax = sns.violinplot(y='value',x='feature', data=data,order=['category','color','firstLetter', 'location','size','wordLength','temporal'])\n",
    "plt.ylabel('Clustering score')\n",
    "plt.ylim(.3,1)\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig('avgFingerprint_exp1.pdf',format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avg fingerprint for exp 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "exp2 = fingerprintsDF['experiment']=='2.1'\n",
    "data = fingerprintsDF[exp2].groupby(['subId','experiment','feature']).mean().reset_index(level=['experiment','feature'])\n",
    "ax = sns.violinplot(y='value',x='feature', data=data,order=['category','color','firstLetter', 'location','size','wordLength','temporal'])\n",
    "plt.ylabel('Clustering score')\n",
    "plt.ylim(.3,1)\n",
    "\n",
    "#plt.savefig('avgFingerprint_exp2.pdf',format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avg fingerprint for exp 1 and 2 together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "exp1 = fingerprintsDF['experiment']=='1.1'\n",
    "exp2 = fingerprintsDF['experiment']=='2.1'\n",
    "data = fingerprintsDF[exp1 | exp2].groupby(['subId','experiment','feature']).mean().reset_index(level=['experiment','feature'])\n",
    "ax = sns.violinplot(y='value',x='feature',hue='experiment',data=data,split=True,scale=\"count\", inner=\"quartile\",order=['category','color','firstLetter', 'location','size','wordLength','temporal'])\n",
    "plt.ylabel('Clustering score')\n",
    "plt.ylim(.3,1)\n",
    "plt.show()\n",
    "\n",
    "exp1=data['experiment']=='1.1'\n",
    "exp2=data['experiment']=='2.1'\n",
    "\n",
    "for feature in ['category','color','firstLetter','location','size','temporal','wordLength']:\n",
    "    featuren=data['feature']==feature\n",
    "    data1=data[exp1&featuren]['value']\n",
    "    data2=data[exp2&featuren]['value']\n",
    "    print(feature + ' ttest (two-sided)',ttest(data1,data2))\n",
    "    \n",
    "# plt.savefig('avgFingerprint_exp1&2.pdf',format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avg fingerprint for exp 1 and 2 together - first half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "exp1 = fingerprintsDF['experiment']=='1.1'\n",
    "exp2 = fingerprintsDF['experiment']=='2.1'\n",
    "firsthalf = fingerprintsDF['listNum']<8\n",
    "data = fingerprintsDF[(exp1 | exp2) & firsthalf].groupby(['subId','experiment','feature']).mean().reset_index(level=['experiment','feature'])\n",
    "ax = sns.violinplot(y='value',x='feature',hue='experiment',data=data,split=True,scale=\"count\", inner=\"quartile\",order=['category','color','firstLetter', 'location','size','wordLength','temporal'])\n",
    "plt.ylabel('Clustering score')\n",
    "plt.ylim(.3,1)\n",
    "plt.show()\n",
    "\n",
    "exp1=data['experiment']=='1.1'\n",
    "exp2=data['experiment']=='3.2'\n",
    "\n",
    "for feature in ['category','color','firstLetter','location','size','temporal','wordLength']:\n",
    "    featuren=data['feature']==feature\n",
    "    data1=data[exp1&featuren]['value']\n",
    "    data2=data[exp2&featuren]['value']\n",
    "    print(feature + ' ttest (two-sided)',ttest(data1,data2))\n",
    "    \n",
    "# plt.savefig('avgFingerprint_exp1&2_firstHalf.pdf',format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avg fingerprint for exp 1 and 2 together - second half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "exp1 = fingerprintsDF['experiment']=='1.1'\n",
    "exp2 = fingerprintsDF['experiment']=='2.1'\n",
    "secondhalf = fingerprintsDF['listNum']>7\n",
    "data = fingerprintsDF[(exp1 | exp2) & secondhalf].groupby(['subId','experiment','feature']).mean().reset_index(level=['experiment','feature'])\n",
    "ax = sns.violinplot(y='value',x='feature',hue='experiment',data=data,split=True,scale=\"count\", inner=\"quartile\",order=['category','color','firstLetter', 'location','size','wordLength','temporal'])\n",
    "plt.ylabel('Clustering score')\n",
    "plt.ylim(.3,1)\n",
    "plt.show()\n",
    "\n",
    "exp1=data['experiment']=='1.1'\n",
    "exp2=data['experiment']=='3.2'\n",
    "\n",
    "for feature in ['category','color','firstLetter','location','size','temporal','wordLength']:\n",
    "    featuren=data['feature']==feature\n",
    "    data1=data[exp1&featuren]['value']\n",
    "    data2=data[exp2&featuren]['value']\n",
    "    print(feature + ' ttest (two-sided)',ttest(data1,data2))\n",
    "    \n",
    "# plt.savefig('avgFingerprint_exp1&2_secondHalf.pdf',format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avg fingerprint for exp 1 and 2 together - second half - split by accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "exp1 = fingerprintsDF['experiment']=='1.1'\n",
    "exp2 = fingerprintsDF['experiment']=='2.1'\n",
    "secondhalf = fingerprintsDF['listNum']>7\n",
    "data = fingerprintsDF[(exp1) & secondhalf].groupby(['subId','experiment','feature']).mean().reset_index(level=['experiment','feature'])\n",
    "accMean = fingerprintsDF[(exp1) & secondhalf].groupby(['subId']).mean()['accuracy'].mean()\n",
    "\n",
    "def f(row):\n",
    "    if row['accuracy'] > accMean:\n",
    "        val = 'high'\n",
    "    else:\n",
    "        val = 'low'\n",
    "    return val\n",
    "\n",
    "data['accSplit'] = data.apply(f, axis=1)\n",
    "\n",
    "ax = sns.violinplot(y='value',x='feature',hue='accSplit',hue_order=['low','high'],data=data,split=True, scale=\"count\", inner=\"quartile\",order=['category','color','firstLetter', 'location','size','wordLength','temporal'])\n",
    "plt.ylabel('Clustering score')\n",
    "plt.ylim(.3,1)\n",
    "plt.show()\n",
    "\n",
    "# high=data['accSplit']=='high'\n",
    "# low=data['accSplit']=='low'\n",
    "\n",
    "# for feature in ['category','color','firstLetter','location','size','temporal','wordLength']:\n",
    "#     featuren=data['feature']==feature\n",
    "#     data1=data[high&featuren]['value']\n",
    "#     data2=data[low&featuren]['value']\n",
    "#     print(feature + ' ttest (two-sided)',ttest(data1,data2))\n",
    "\n",
    "# plt.savefig('avgFingerprint_exp1_secondHalf_accSplit.pdf',format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avg fingerprint for exp 1 and 2 together - second half - split by accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "exp1 = fingerprintsDF['experiment']=='1.1'\n",
    "exp2 = fingerprintsDF['experiment']=='2.1'\n",
    "secondhalf = fingerprintsDF['listNum']<8\n",
    "data = fingerprintsDF[(exp2) & secondhalf].groupby(['subId','experiment','feature']).mean().reset_index(level=['experiment','feature'])\n",
    "accMean = fingerprintsDF[(exp2) & secondhalf].groupby(['subId']).mean()['accuracy'].mean()\n",
    "\n",
    "def f(row):\n",
    "    if row['accuracy'] > accMean:\n",
    "        val = 'high'\n",
    "    else:\n",
    "        val = 'low'\n",
    "    return val\n",
    "\n",
    "data['accSplit'] = data.apply(f, axis=1)\n",
    "\n",
    "ax = sns.violinplot(y='value',x='feature',hue='accSplit',hue_order=['low','high'],data=data,split=True, scale=\"count\", inner=\"quartile\",order=['category','color','firstLetter', 'location','size','wordLength','temporal'])\n",
    "plt.ylabel('Clustering score')\n",
    "plt.ylim(.3,1)\n",
    "plt.show()\n",
    "\n",
    "# high=data['accSplit']=='high'\n",
    "# low=data['accSplit']=='low'\n",
    "\n",
    "# for feature in ['category','color','firstLetter','location','size','temporal','wordLength']:\n",
    "#     featuren=data['feature']==feature\n",
    "#     data1=data[high&featuren]['value']\n",
    "#     data2=data[low&featuren]['value']\n",
    "#     print(feature + ' ttest (two-sided)',ttest(data1,data2))\n",
    "\n",
    "#plt.savefig('avgFingerprint_exp2_firstHalf_accSplit.pdf',format='pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avg fingerprint for exp 1 and 2 together - second half - split by accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "exp1 = fingerprintsDF['experiment']=='1.1'\n",
    "exp2 = fingerprintsDF['experiment']=='2.1'\n",
    "def f(row):\n",
    "    if row['listNum'] < 8 :\n",
    "        val = 'first half'\n",
    "    else:\n",
    "        val = 'second half'\n",
    "    return val\n",
    "\n",
    "fingerprintsDF['time'] = fingerprintsDF.apply(f, axis=1)\n",
    "\n",
    "data = fingerprintsDF[exp2].groupby(['subId','time','feature']).mean().reset_index(level=['feature','time'])\n",
    "ax = sns.violinplot(y='value',x='feature',hue='time',hue_order=['first half','second half'],data=data,split=True, scale=\"count\", inner=\"quartile\",order=['category','color','firstLetter', 'location','size','wordLength','temporal'])\n",
    "plt.ylabel('Clustering score')\n",
    "plt.ylim(.3,1)\n",
    "# plt.show()\n",
    "\n",
    "# plt.savefig('avgFingerprint_exp2_splitByTime.pdf',format='pdf')\n",
    "# plt.show()\n",
    "# # high=data['accSplit']=='high'\n",
    "# # low=data['accSplit']=='low'\n",
    "\n",
    "# # for feature in ['category','color','firstLetter','location','size','temporal','wordLength']:\n",
    "# #     featuren=data['feature']==feature\n",
    "# #     data1=data[high&featuren]['value']\n",
    "# #     data2=data[low&featuren]['value']\n",
    "# #     print(feature + ' ttest (two-sided)',ttest(data1,data2))\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# exp1 = fingerprintsDF['experiment']=='1.1'\n",
    "# exp2 = fingerprintsDF['experiment']=='2.1'\n",
    "# secondHalf = fingerprintsDF['listNum']>7\n",
    "# feature=fingerprintsDF['feature']=='temporal'\n",
    "# data = fingerprintsDF[exp1 & feature].groupby(['subId','feature']).mean().reset_index(level=['feature'])\n",
    "# data = data[['feature','accuracy','value']]\n",
    "\n",
    "# g = sns.pairplot(data,hue='feature')\n",
    "# plt.show()\n",
    "\n",
    "# feature='temporal'\n",
    "# a=data[data['feature']==feature]['accuracy']\n",
    "# b=data[data['feature']==feature]['value']\n",
    "# import scipy\n",
    "# print(scipy.stats.pearsonr(a,b))\n",
    "\n",
    "# corrs={}\n",
    "# corrs['category']=0.43111982873068738\n",
    "# corrs['color']=0.042551908675165041\n",
    "# corrs['size']=0.56232253397336707\n",
    "# corrs['location']=-0.15929961652715122\n",
    "# corrs['wordLength']=0.057218271958420434\n",
    "# corrs['firstLetter']=0.50747948723671299\n",
    "# corrs['temporal']=-0.52346593256870289\n",
    "\n",
    "# x=['category','color','size','location','wordLength','firstLetter','temporal']\n",
    "# y=['clustering score / recall accuracy correlation']\n",
    "# data = pd.Series(corrs)\n",
    "# sns.barplot(data=data,y=[key for key in corrs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
